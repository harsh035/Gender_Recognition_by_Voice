{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import basic libraries\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.064241</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.090193</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>12.863462</td>\n",
       "      <td>274.402906</td>\n",
       "      <td>0.893369</td>\n",
       "      <td>0.491918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.067310</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>22.423285</td>\n",
       "      <td>634.613855</td>\n",
       "      <td>0.892193</td>\n",
       "      <td>0.513724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.083829</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.131908</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>30.757155</td>\n",
       "      <td>1024.927705</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.158011</td>\n",
       "      <td>0.096582</td>\n",
       "      <td>0.207955</td>\n",
       "      <td>0.111374</td>\n",
       "      <td>1.232831</td>\n",
       "      <td>4.177296</td>\n",
       "      <td>0.963322</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.247119</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.079146</td>\n",
       "      <td>0.124656</td>\n",
       "      <td>0.078720</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.127325</td>\n",
       "      <td>1.101174</td>\n",
       "      <td>4.333713</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0.783568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.712812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>5.484375</td>\n",
       "      <td>5.476562</td>\n",
       "      <td>0.208274</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
       "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
       "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
       "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
       "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
       "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
       "\n",
       "          kurt    sp.ent       sfm  ...    centroid   meanfun    minfun  \\\n",
       "0   274.402906  0.893369  0.491918  ...    0.059781  0.084279  0.015702   \n",
       "1   634.613855  0.892193  0.513724  ...    0.066009  0.107937  0.015826   \n",
       "2  1024.927705  0.846389  0.478905  ...    0.077316  0.098706  0.015656   \n",
       "3     4.177296  0.963322  0.727232  ...    0.151228  0.088965  0.017798   \n",
       "4     4.333713  0.971955  0.783568  ...    0.135120  0.106398  0.016931   \n",
       "\n",
       "     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000   male  \n",
       "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632   male  \n",
       "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512   male  \n",
       "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119   male  \n",
       "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274   male  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "dataset = pd.read_csv('voice.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel will use an artificial neural net to classify voices by gender. The neural net will have two hidden layers, will use a rectifier activation function for each node in the hidden layers, and use a logistic activation function to recieve probabilities of each class in the output layer and will use stochastic gradient descent to minimize the objective function (cross entropy). We begin by reading the dataset and do some elementary data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>mode</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.180907</td>\n",
       "      <td>0.057126</td>\n",
       "      <td>0.185621</td>\n",
       "      <td>0.140456</td>\n",
       "      <td>0.224765</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>3.140168</td>\n",
       "      <td>36.568461</td>\n",
       "      <td>0.895127</td>\n",
       "      <td>0.408216</td>\n",
       "      <td>0.165282</td>\n",
       "      <td>0.180907</td>\n",
       "      <td>0.142807</td>\n",
       "      <td>0.036802</td>\n",
       "      <td>0.258842</td>\n",
       "      <td>0.829211</td>\n",
       "      <td>0.052647</td>\n",
       "      <td>5.047277</td>\n",
       "      <td>4.994630</td>\n",
       "      <td>0.173752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.029918</td>\n",
       "      <td>0.016652</td>\n",
       "      <td>0.036360</td>\n",
       "      <td>0.048680</td>\n",
       "      <td>0.023639</td>\n",
       "      <td>0.042783</td>\n",
       "      <td>4.240529</td>\n",
       "      <td>134.928661</td>\n",
       "      <td>0.044980</td>\n",
       "      <td>0.177521</td>\n",
       "      <td>0.077203</td>\n",
       "      <td>0.029918</td>\n",
       "      <td>0.032304</td>\n",
       "      <td>0.019220</td>\n",
       "      <td>0.030077</td>\n",
       "      <td>0.525205</td>\n",
       "      <td>0.063299</td>\n",
       "      <td>3.521157</td>\n",
       "      <td>3.520039</td>\n",
       "      <td>0.119454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.039363</td>\n",
       "      <td>0.018363</td>\n",
       "      <td>0.010975</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.042946</td>\n",
       "      <td>0.014558</td>\n",
       "      <td>0.141735</td>\n",
       "      <td>2.068455</td>\n",
       "      <td>0.738651</td>\n",
       "      <td>0.036876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039363</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>0.009775</td>\n",
       "      <td>0.103093</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.163662</td>\n",
       "      <td>0.041954</td>\n",
       "      <td>0.169593</td>\n",
       "      <td>0.111087</td>\n",
       "      <td>0.208747</td>\n",
       "      <td>0.042560</td>\n",
       "      <td>1.649569</td>\n",
       "      <td>5.669547</td>\n",
       "      <td>0.861811</td>\n",
       "      <td>0.258041</td>\n",
       "      <td>0.118016</td>\n",
       "      <td>0.163662</td>\n",
       "      <td>0.116998</td>\n",
       "      <td>0.018223</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.419828</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>2.070312</td>\n",
       "      <td>2.044922</td>\n",
       "      <td>0.099766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.184838</td>\n",
       "      <td>0.059155</td>\n",
       "      <td>0.190032</td>\n",
       "      <td>0.140286</td>\n",
       "      <td>0.225684</td>\n",
       "      <td>0.094280</td>\n",
       "      <td>2.197101</td>\n",
       "      <td>8.318463</td>\n",
       "      <td>0.901767</td>\n",
       "      <td>0.396335</td>\n",
       "      <td>0.186599</td>\n",
       "      <td>0.184838</td>\n",
       "      <td>0.140519</td>\n",
       "      <td>0.046110</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.765795</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>4.992188</td>\n",
       "      <td>4.945312</td>\n",
       "      <td>0.139357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.199146</td>\n",
       "      <td>0.067020</td>\n",
       "      <td>0.210618</td>\n",
       "      <td>0.175939</td>\n",
       "      <td>0.243660</td>\n",
       "      <td>0.114175</td>\n",
       "      <td>2.931694</td>\n",
       "      <td>13.648905</td>\n",
       "      <td>0.928713</td>\n",
       "      <td>0.533676</td>\n",
       "      <td>0.221104</td>\n",
       "      <td>0.199146</td>\n",
       "      <td>0.169581</td>\n",
       "      <td>0.047904</td>\n",
       "      <td>0.277457</td>\n",
       "      <td>1.177166</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>7.007812</td>\n",
       "      <td>6.992188</td>\n",
       "      <td>0.209183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.251124</td>\n",
       "      <td>0.115273</td>\n",
       "      <td>0.261224</td>\n",
       "      <td>0.247347</td>\n",
       "      <td>0.273469</td>\n",
       "      <td>0.252225</td>\n",
       "      <td>34.725453</td>\n",
       "      <td>1309.612887</td>\n",
       "      <td>0.981997</td>\n",
       "      <td>0.842936</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.251124</td>\n",
       "      <td>0.237636</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.279114</td>\n",
       "      <td>2.957682</td>\n",
       "      <td>0.458984</td>\n",
       "      <td>21.867188</td>\n",
       "      <td>21.843750</td>\n",
       "      <td>0.932374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          meanfreq           sd       median          Q25          Q75  \\\n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000   \n",
       "mean      0.180907     0.057126     0.185621     0.140456     0.224765   \n",
       "std       0.029918     0.016652     0.036360     0.048680     0.023639   \n",
       "min       0.039363     0.018363     0.010975     0.000229     0.042946   \n",
       "25%       0.163662     0.041954     0.169593     0.111087     0.208747   \n",
       "50%       0.184838     0.059155     0.190032     0.140286     0.225684   \n",
       "75%       0.199146     0.067020     0.210618     0.175939     0.243660   \n",
       "max       0.251124     0.115273     0.261224     0.247347     0.273469   \n",
       "\n",
       "               IQR         skew         kurt       sp.ent          sfm  \\\n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000   \n",
       "mean      0.084309     3.140168    36.568461     0.895127     0.408216   \n",
       "std       0.042783     4.240529   134.928661     0.044980     0.177521   \n",
       "min       0.014558     0.141735     2.068455     0.738651     0.036876   \n",
       "25%       0.042560     1.649569     5.669547     0.861811     0.258041   \n",
       "50%       0.094280     2.197101     8.318463     0.901767     0.396335   \n",
       "75%       0.114175     2.931694    13.648905     0.928713     0.533676   \n",
       "max       0.252225    34.725453  1309.612887     0.981997     0.842936   \n",
       "\n",
       "              mode     centroid      meanfun       minfun       maxfun  \\\n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000   \n",
       "mean      0.165282     0.180907     0.142807     0.036802     0.258842   \n",
       "std       0.077203     0.029918     0.032304     0.019220     0.030077   \n",
       "min       0.000000     0.039363     0.055565     0.009775     0.103093   \n",
       "25%       0.118016     0.163662     0.116998     0.018223     0.253968   \n",
       "50%       0.186599     0.184838     0.140519     0.046110     0.271186   \n",
       "75%       0.221104     0.199146     0.169581     0.047904     0.277457   \n",
       "max       0.280000     0.251124     0.237636     0.204082     0.279114   \n",
       "\n",
       "           meandom       mindom       maxdom      dfrange      modindx  \n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000  \n",
       "mean      0.829211     0.052647     5.047277     4.994630     0.173752  \n",
       "std       0.525205     0.063299     3.521157     3.520039     0.119454  \n",
       "min       0.007812     0.004883     0.007812     0.000000     0.000000  \n",
       "25%       0.419828     0.007812     2.070312     2.044922     0.099766  \n",
       "50%       0.765795     0.023438     4.992188     4.945312     0.139357  \n",
       "75%       1.177166     0.070312     7.007812     6.992188     0.209183  \n",
       "max       2.957682     0.458984    21.867188    21.843750     0.932374  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's Describe the dataset\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>mode</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>meanfreq</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.739039</td>\n",
       "      <td>0.925445</td>\n",
       "      <td>0.911416</td>\n",
       "      <td>0.740997</td>\n",
       "      <td>-0.627605</td>\n",
       "      <td>-0.322327</td>\n",
       "      <td>-0.316036</td>\n",
       "      <td>-0.601203</td>\n",
       "      <td>-0.784332</td>\n",
       "      <td>0.687715</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460844</td>\n",
       "      <td>0.383937</td>\n",
       "      <td>0.274004</td>\n",
       "      <td>0.536666</td>\n",
       "      <td>0.229261</td>\n",
       "      <td>0.519528</td>\n",
       "      <td>0.515570</td>\n",
       "      <td>-0.216979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sd</th>\n",
       "      <td>-0.739039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.562603</td>\n",
       "      <td>-0.846931</td>\n",
       "      <td>-0.161076</td>\n",
       "      <td>0.874660</td>\n",
       "      <td>0.314597</td>\n",
       "      <td>0.346241</td>\n",
       "      <td>0.716620</td>\n",
       "      <td>0.838086</td>\n",
       "      <td>-0.529150</td>\n",
       "      <td>-0.739039</td>\n",
       "      <td>-0.466281</td>\n",
       "      <td>-0.345609</td>\n",
       "      <td>-0.129662</td>\n",
       "      <td>-0.482726</td>\n",
       "      <td>-0.357667</td>\n",
       "      <td>-0.482278</td>\n",
       "      <td>-0.475999</td>\n",
       "      <td>0.122660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>0.925445</td>\n",
       "      <td>-0.562603</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774922</td>\n",
       "      <td>0.731849</td>\n",
       "      <td>-0.477352</td>\n",
       "      <td>-0.257407</td>\n",
       "      <td>-0.243382</td>\n",
       "      <td>-0.502005</td>\n",
       "      <td>-0.661690</td>\n",
       "      <td>0.677433</td>\n",
       "      <td>0.925445</td>\n",
       "      <td>0.414909</td>\n",
       "      <td>0.337602</td>\n",
       "      <td>0.251328</td>\n",
       "      <td>0.455943</td>\n",
       "      <td>0.191169</td>\n",
       "      <td>0.438919</td>\n",
       "      <td>0.435621</td>\n",
       "      <td>-0.213298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q25</th>\n",
       "      <td>0.911416</td>\n",
       "      <td>-0.846931</td>\n",
       "      <td>0.774922</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.477140</td>\n",
       "      <td>-0.874189</td>\n",
       "      <td>-0.319475</td>\n",
       "      <td>-0.350182</td>\n",
       "      <td>-0.648126</td>\n",
       "      <td>-0.766875</td>\n",
       "      <td>0.591277</td>\n",
       "      <td>0.911416</td>\n",
       "      <td>0.545035</td>\n",
       "      <td>0.320994</td>\n",
       "      <td>0.199841</td>\n",
       "      <td>0.467403</td>\n",
       "      <td>0.302255</td>\n",
       "      <td>0.459683</td>\n",
       "      <td>0.454394</td>\n",
       "      <td>-0.141377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q75</th>\n",
       "      <td>0.740997</td>\n",
       "      <td>-0.161076</td>\n",
       "      <td>0.731849</td>\n",
       "      <td>0.477140</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>-0.206339</td>\n",
       "      <td>-0.148881</td>\n",
       "      <td>-0.174905</td>\n",
       "      <td>-0.378198</td>\n",
       "      <td>0.486857</td>\n",
       "      <td>0.740997</td>\n",
       "      <td>0.155091</td>\n",
       "      <td>0.258002</td>\n",
       "      <td>0.285584</td>\n",
       "      <td>0.359181</td>\n",
       "      <td>-0.023750</td>\n",
       "      <td>0.335114</td>\n",
       "      <td>0.335648</td>\n",
       "      <td>-0.216475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IQR</th>\n",
       "      <td>-0.627605</td>\n",
       "      <td>0.874660</td>\n",
       "      <td>-0.477352</td>\n",
       "      <td>-0.874189</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.249497</td>\n",
       "      <td>0.316185</td>\n",
       "      <td>0.640813</td>\n",
       "      <td>0.663601</td>\n",
       "      <td>-0.403764</td>\n",
       "      <td>-0.627605</td>\n",
       "      <td>-0.534462</td>\n",
       "      <td>-0.222680</td>\n",
       "      <td>-0.069588</td>\n",
       "      <td>-0.333362</td>\n",
       "      <td>-0.357037</td>\n",
       "      <td>-0.337877</td>\n",
       "      <td>-0.331563</td>\n",
       "      <td>0.041252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skew</th>\n",
       "      <td>-0.322327</td>\n",
       "      <td>0.314597</td>\n",
       "      <td>-0.257407</td>\n",
       "      <td>-0.319475</td>\n",
       "      <td>-0.206339</td>\n",
       "      <td>0.249497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977020</td>\n",
       "      <td>-0.195459</td>\n",
       "      <td>0.079694</td>\n",
       "      <td>-0.434859</td>\n",
       "      <td>-0.322327</td>\n",
       "      <td>-0.167668</td>\n",
       "      <td>-0.216954</td>\n",
       "      <td>-0.080861</td>\n",
       "      <td>-0.336848</td>\n",
       "      <td>-0.061608</td>\n",
       "      <td>-0.305651</td>\n",
       "      <td>-0.304640</td>\n",
       "      <td>-0.169325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kurt</th>\n",
       "      <td>-0.316036</td>\n",
       "      <td>0.346241</td>\n",
       "      <td>-0.243382</td>\n",
       "      <td>-0.350182</td>\n",
       "      <td>-0.148881</td>\n",
       "      <td>0.316185</td>\n",
       "      <td>0.977020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.127644</td>\n",
       "      <td>0.109884</td>\n",
       "      <td>-0.406722</td>\n",
       "      <td>-0.316036</td>\n",
       "      <td>-0.194560</td>\n",
       "      <td>-0.203201</td>\n",
       "      <td>-0.045667</td>\n",
       "      <td>-0.303234</td>\n",
       "      <td>-0.103313</td>\n",
       "      <td>-0.274500</td>\n",
       "      <td>-0.272729</td>\n",
       "      <td>-0.205539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp.ent</th>\n",
       "      <td>-0.601203</td>\n",
       "      <td>0.716620</td>\n",
       "      <td>-0.502005</td>\n",
       "      <td>-0.648126</td>\n",
       "      <td>-0.174905</td>\n",
       "      <td>0.640813</td>\n",
       "      <td>-0.195459</td>\n",
       "      <td>-0.127644</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866411</td>\n",
       "      <td>-0.325298</td>\n",
       "      <td>-0.601203</td>\n",
       "      <td>-0.513194</td>\n",
       "      <td>-0.305826</td>\n",
       "      <td>-0.120738</td>\n",
       "      <td>-0.293562</td>\n",
       "      <td>-0.294869</td>\n",
       "      <td>-0.324253</td>\n",
       "      <td>-0.319054</td>\n",
       "      <td>0.198074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sfm</th>\n",
       "      <td>-0.784332</td>\n",
       "      <td>0.838086</td>\n",
       "      <td>-0.661690</td>\n",
       "      <td>-0.766875</td>\n",
       "      <td>-0.378198</td>\n",
       "      <td>0.663601</td>\n",
       "      <td>0.079694</td>\n",
       "      <td>0.109884</td>\n",
       "      <td>0.866411</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.485913</td>\n",
       "      <td>-0.784332</td>\n",
       "      <td>-0.421066</td>\n",
       "      <td>-0.362100</td>\n",
       "      <td>-0.192369</td>\n",
       "      <td>-0.428442</td>\n",
       "      <td>-0.289593</td>\n",
       "      <td>-0.436649</td>\n",
       "      <td>-0.431580</td>\n",
       "      <td>0.211477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mode</th>\n",
       "      <td>0.687715</td>\n",
       "      <td>-0.529150</td>\n",
       "      <td>0.677433</td>\n",
       "      <td>0.591277</td>\n",
       "      <td>0.486857</td>\n",
       "      <td>-0.403764</td>\n",
       "      <td>-0.434859</td>\n",
       "      <td>-0.406722</td>\n",
       "      <td>-0.325298</td>\n",
       "      <td>-0.485913</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.687715</td>\n",
       "      <td>0.324771</td>\n",
       "      <td>0.385467</td>\n",
       "      <td>0.172329</td>\n",
       "      <td>0.491479</td>\n",
       "      <td>0.198150</td>\n",
       "      <td>0.477187</td>\n",
       "      <td>0.473775</td>\n",
       "      <td>-0.182344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>centroid</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.739039</td>\n",
       "      <td>0.925445</td>\n",
       "      <td>0.911416</td>\n",
       "      <td>0.740997</td>\n",
       "      <td>-0.627605</td>\n",
       "      <td>-0.322327</td>\n",
       "      <td>-0.316036</td>\n",
       "      <td>-0.601203</td>\n",
       "      <td>-0.784332</td>\n",
       "      <td>0.687715</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460844</td>\n",
       "      <td>0.383937</td>\n",
       "      <td>0.274004</td>\n",
       "      <td>0.536666</td>\n",
       "      <td>0.229261</td>\n",
       "      <td>0.519528</td>\n",
       "      <td>0.515570</td>\n",
       "      <td>-0.216979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meanfun</th>\n",
       "      <td>0.460844</td>\n",
       "      <td>-0.466281</td>\n",
       "      <td>0.414909</td>\n",
       "      <td>0.545035</td>\n",
       "      <td>0.155091</td>\n",
       "      <td>-0.534462</td>\n",
       "      <td>-0.167668</td>\n",
       "      <td>-0.194560</td>\n",
       "      <td>-0.513194</td>\n",
       "      <td>-0.421066</td>\n",
       "      <td>0.324771</td>\n",
       "      <td>0.460844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339387</td>\n",
       "      <td>0.311950</td>\n",
       "      <td>0.270840</td>\n",
       "      <td>0.162163</td>\n",
       "      <td>0.277982</td>\n",
       "      <td>0.275154</td>\n",
       "      <td>-0.054858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minfun</th>\n",
       "      <td>0.383937</td>\n",
       "      <td>-0.345609</td>\n",
       "      <td>0.337602</td>\n",
       "      <td>0.320994</td>\n",
       "      <td>0.258002</td>\n",
       "      <td>-0.222680</td>\n",
       "      <td>-0.216954</td>\n",
       "      <td>-0.203201</td>\n",
       "      <td>-0.305826</td>\n",
       "      <td>-0.362100</td>\n",
       "      <td>0.385467</td>\n",
       "      <td>0.383937</td>\n",
       "      <td>0.339387</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.213987</td>\n",
       "      <td>0.375979</td>\n",
       "      <td>0.082015</td>\n",
       "      <td>0.317860</td>\n",
       "      <td>0.316486</td>\n",
       "      <td>0.002042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maxfun</th>\n",
       "      <td>0.274004</td>\n",
       "      <td>-0.129662</td>\n",
       "      <td>0.251328</td>\n",
       "      <td>0.199841</td>\n",
       "      <td>0.285584</td>\n",
       "      <td>-0.069588</td>\n",
       "      <td>-0.080861</td>\n",
       "      <td>-0.045667</td>\n",
       "      <td>-0.120738</td>\n",
       "      <td>-0.192369</td>\n",
       "      <td>0.172329</td>\n",
       "      <td>0.274004</td>\n",
       "      <td>0.311950</td>\n",
       "      <td>0.213987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337553</td>\n",
       "      <td>-0.243426</td>\n",
       "      <td>0.355390</td>\n",
       "      <td>0.359880</td>\n",
       "      <td>-0.363029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meandom</th>\n",
       "      <td>0.536666</td>\n",
       "      <td>-0.482726</td>\n",
       "      <td>0.455943</td>\n",
       "      <td>0.467403</td>\n",
       "      <td>0.359181</td>\n",
       "      <td>-0.333362</td>\n",
       "      <td>-0.336848</td>\n",
       "      <td>-0.303234</td>\n",
       "      <td>-0.293562</td>\n",
       "      <td>-0.428442</td>\n",
       "      <td>0.491479</td>\n",
       "      <td>0.536666</td>\n",
       "      <td>0.270840</td>\n",
       "      <td>0.375979</td>\n",
       "      <td>0.337553</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099656</td>\n",
       "      <td>0.812838</td>\n",
       "      <td>0.811304</td>\n",
       "      <td>-0.180954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mindom</th>\n",
       "      <td>0.229261</td>\n",
       "      <td>-0.357667</td>\n",
       "      <td>0.191169</td>\n",
       "      <td>0.302255</td>\n",
       "      <td>-0.023750</td>\n",
       "      <td>-0.357037</td>\n",
       "      <td>-0.061608</td>\n",
       "      <td>-0.103313</td>\n",
       "      <td>-0.294869</td>\n",
       "      <td>-0.289593</td>\n",
       "      <td>0.198150</td>\n",
       "      <td>0.229261</td>\n",
       "      <td>0.162163</td>\n",
       "      <td>0.082015</td>\n",
       "      <td>-0.243426</td>\n",
       "      <td>0.099656</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026640</td>\n",
       "      <td>0.008666</td>\n",
       "      <td>0.200212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maxdom</th>\n",
       "      <td>0.519528</td>\n",
       "      <td>-0.482278</td>\n",
       "      <td>0.438919</td>\n",
       "      <td>0.459683</td>\n",
       "      <td>0.335114</td>\n",
       "      <td>-0.337877</td>\n",
       "      <td>-0.305651</td>\n",
       "      <td>-0.274500</td>\n",
       "      <td>-0.324253</td>\n",
       "      <td>-0.436649</td>\n",
       "      <td>0.477187</td>\n",
       "      <td>0.519528</td>\n",
       "      <td>0.277982</td>\n",
       "      <td>0.317860</td>\n",
       "      <td>0.355390</td>\n",
       "      <td>0.812838</td>\n",
       "      <td>0.026640</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999838</td>\n",
       "      <td>-0.425531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dfrange</th>\n",
       "      <td>0.515570</td>\n",
       "      <td>-0.475999</td>\n",
       "      <td>0.435621</td>\n",
       "      <td>0.454394</td>\n",
       "      <td>0.335648</td>\n",
       "      <td>-0.331563</td>\n",
       "      <td>-0.304640</td>\n",
       "      <td>-0.272729</td>\n",
       "      <td>-0.319054</td>\n",
       "      <td>-0.431580</td>\n",
       "      <td>0.473775</td>\n",
       "      <td>0.515570</td>\n",
       "      <td>0.275154</td>\n",
       "      <td>0.316486</td>\n",
       "      <td>0.359880</td>\n",
       "      <td>0.811304</td>\n",
       "      <td>0.008666</td>\n",
       "      <td>0.999838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.429266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modindx</th>\n",
       "      <td>-0.216979</td>\n",
       "      <td>0.122660</td>\n",
       "      <td>-0.213298</td>\n",
       "      <td>-0.141377</td>\n",
       "      <td>-0.216475</td>\n",
       "      <td>0.041252</td>\n",
       "      <td>-0.169325</td>\n",
       "      <td>-0.205539</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.211477</td>\n",
       "      <td>-0.182344</td>\n",
       "      <td>-0.216979</td>\n",
       "      <td>-0.054858</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>-0.363029</td>\n",
       "      <td>-0.180954</td>\n",
       "      <td>0.200212</td>\n",
       "      <td>-0.425531</td>\n",
       "      <td>-0.429266</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          meanfreq        sd    median       Q25       Q75       IQR  \\\n",
       "meanfreq  1.000000 -0.739039  0.925445  0.911416  0.740997 -0.627605   \n",
       "sd       -0.739039  1.000000 -0.562603 -0.846931 -0.161076  0.874660   \n",
       "median    0.925445 -0.562603  1.000000  0.774922  0.731849 -0.477352   \n",
       "Q25       0.911416 -0.846931  0.774922  1.000000  0.477140 -0.874189   \n",
       "Q75       0.740997 -0.161076  0.731849  0.477140  1.000000  0.009636   \n",
       "IQR      -0.627605  0.874660 -0.477352 -0.874189  0.009636  1.000000   \n",
       "skew     -0.322327  0.314597 -0.257407 -0.319475 -0.206339  0.249497   \n",
       "kurt     -0.316036  0.346241 -0.243382 -0.350182 -0.148881  0.316185   \n",
       "sp.ent   -0.601203  0.716620 -0.502005 -0.648126 -0.174905  0.640813   \n",
       "sfm      -0.784332  0.838086 -0.661690 -0.766875 -0.378198  0.663601   \n",
       "mode      0.687715 -0.529150  0.677433  0.591277  0.486857 -0.403764   \n",
       "centroid  1.000000 -0.739039  0.925445  0.911416  0.740997 -0.627605   \n",
       "meanfun   0.460844 -0.466281  0.414909  0.545035  0.155091 -0.534462   \n",
       "minfun    0.383937 -0.345609  0.337602  0.320994  0.258002 -0.222680   \n",
       "maxfun    0.274004 -0.129662  0.251328  0.199841  0.285584 -0.069588   \n",
       "meandom   0.536666 -0.482726  0.455943  0.467403  0.359181 -0.333362   \n",
       "mindom    0.229261 -0.357667  0.191169  0.302255 -0.023750 -0.357037   \n",
       "maxdom    0.519528 -0.482278  0.438919  0.459683  0.335114 -0.337877   \n",
       "dfrange   0.515570 -0.475999  0.435621  0.454394  0.335648 -0.331563   \n",
       "modindx  -0.216979  0.122660 -0.213298 -0.141377 -0.216475  0.041252   \n",
       "\n",
       "              skew      kurt    sp.ent       sfm      mode  centroid  \\\n",
       "meanfreq -0.322327 -0.316036 -0.601203 -0.784332  0.687715  1.000000   \n",
       "sd        0.314597  0.346241  0.716620  0.838086 -0.529150 -0.739039   \n",
       "median   -0.257407 -0.243382 -0.502005 -0.661690  0.677433  0.925445   \n",
       "Q25      -0.319475 -0.350182 -0.648126 -0.766875  0.591277  0.911416   \n",
       "Q75      -0.206339 -0.148881 -0.174905 -0.378198  0.486857  0.740997   \n",
       "IQR       0.249497  0.316185  0.640813  0.663601 -0.403764 -0.627605   \n",
       "skew      1.000000  0.977020 -0.195459  0.079694 -0.434859 -0.322327   \n",
       "kurt      0.977020  1.000000 -0.127644  0.109884 -0.406722 -0.316036   \n",
       "sp.ent   -0.195459 -0.127644  1.000000  0.866411 -0.325298 -0.601203   \n",
       "sfm       0.079694  0.109884  0.866411  1.000000 -0.485913 -0.784332   \n",
       "mode     -0.434859 -0.406722 -0.325298 -0.485913  1.000000  0.687715   \n",
       "centroid -0.322327 -0.316036 -0.601203 -0.784332  0.687715  1.000000   \n",
       "meanfun  -0.167668 -0.194560 -0.513194 -0.421066  0.324771  0.460844   \n",
       "minfun   -0.216954 -0.203201 -0.305826 -0.362100  0.385467  0.383937   \n",
       "maxfun   -0.080861 -0.045667 -0.120738 -0.192369  0.172329  0.274004   \n",
       "meandom  -0.336848 -0.303234 -0.293562 -0.428442  0.491479  0.536666   \n",
       "mindom   -0.061608 -0.103313 -0.294869 -0.289593  0.198150  0.229261   \n",
       "maxdom   -0.305651 -0.274500 -0.324253 -0.436649  0.477187  0.519528   \n",
       "dfrange  -0.304640 -0.272729 -0.319054 -0.431580  0.473775  0.515570   \n",
       "modindx  -0.169325 -0.205539  0.198074  0.211477 -0.182344 -0.216979   \n",
       "\n",
       "           meanfun    minfun    maxfun   meandom    mindom    maxdom  \\\n",
       "meanfreq  0.460844  0.383937  0.274004  0.536666  0.229261  0.519528   \n",
       "sd       -0.466281 -0.345609 -0.129662 -0.482726 -0.357667 -0.482278   \n",
       "median    0.414909  0.337602  0.251328  0.455943  0.191169  0.438919   \n",
       "Q25       0.545035  0.320994  0.199841  0.467403  0.302255  0.459683   \n",
       "Q75       0.155091  0.258002  0.285584  0.359181 -0.023750  0.335114   \n",
       "IQR      -0.534462 -0.222680 -0.069588 -0.333362 -0.357037 -0.337877   \n",
       "skew     -0.167668 -0.216954 -0.080861 -0.336848 -0.061608 -0.305651   \n",
       "kurt     -0.194560 -0.203201 -0.045667 -0.303234 -0.103313 -0.274500   \n",
       "sp.ent   -0.513194 -0.305826 -0.120738 -0.293562 -0.294869 -0.324253   \n",
       "sfm      -0.421066 -0.362100 -0.192369 -0.428442 -0.289593 -0.436649   \n",
       "mode      0.324771  0.385467  0.172329  0.491479  0.198150  0.477187   \n",
       "centroid  0.460844  0.383937  0.274004  0.536666  0.229261  0.519528   \n",
       "meanfun   1.000000  0.339387  0.311950  0.270840  0.162163  0.277982   \n",
       "minfun    0.339387  1.000000  0.213987  0.375979  0.082015  0.317860   \n",
       "maxfun    0.311950  0.213987  1.000000  0.337553 -0.243426  0.355390   \n",
       "meandom   0.270840  0.375979  0.337553  1.000000  0.099656  0.812838   \n",
       "mindom    0.162163  0.082015 -0.243426  0.099656  1.000000  0.026640   \n",
       "maxdom    0.277982  0.317860  0.355390  0.812838  0.026640  1.000000   \n",
       "dfrange   0.275154  0.316486  0.359880  0.811304  0.008666  0.999838   \n",
       "modindx  -0.054858  0.002042 -0.363029 -0.180954  0.200212 -0.425531   \n",
       "\n",
       "           dfrange   modindx  \n",
       "meanfreq  0.515570 -0.216979  \n",
       "sd       -0.475999  0.122660  \n",
       "median    0.435621 -0.213298  \n",
       "Q25       0.454394 -0.141377  \n",
       "Q75       0.335648 -0.216475  \n",
       "IQR      -0.331563  0.041252  \n",
       "skew     -0.304640 -0.169325  \n",
       "kurt     -0.272729 -0.205539  \n",
       "sp.ent   -0.319054  0.198074  \n",
       "sfm      -0.431580  0.211477  \n",
       "mode      0.473775 -0.182344  \n",
       "centroid  0.515570 -0.216979  \n",
       "meanfun   0.275154 -0.054858  \n",
       "minfun    0.316486  0.002042  \n",
       "maxfun    0.359880 -0.363029  \n",
       "meandom   0.811304 -0.180954  \n",
       "mindom    0.008666  0.200212  \n",
       "maxdom    0.999838 -0.425531  \n",
       "dfrange   1.000000 -0.429266  \n",
       "modindx  -0.429266  1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the correlation between attributes\n",
    "dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset as per independent and dependent variable\n",
    "num_columns = dataset.shape[1]\n",
    "x = dataset.iloc[:,:20].values\n",
    "y = dataset.iloc[:,20].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one non numeric category (labels) needs to be encoded using dummy variables,when we assign integer values to gender (binary variable in this case), it becomes a dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female', 'male']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kishor/.local/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "gender_labels = LabelEncoder()\n",
    "y = gender_labels.fit_transform(y)\n",
    "# lets see which is 0 and which is 1\n",
    "print(list(gender_labels.inverse_transform([0,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate testing and training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables are between 0 and 1, but we normalize them so that they have comparable weightings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an artificial neural network using keras, by building the layers two hidden layers and an output layer, then train the neural network on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2534/2534 [==============================] - 1s 357us/step - loss: 0.5129 - acc: 0.8106\n",
      "Epoch 2/100\n",
      "2534/2534 [==============================] - 0s 105us/step - loss: 0.1296 - acc: 0.9672\n",
      "Epoch 3/100\n",
      "2534/2534 [==============================] - 0s 104us/step - loss: 0.0899 - acc: 0.9736\n",
      "Epoch 4/100\n",
      "2534/2534 [==============================] - 0s 105us/step - loss: 0.0822 - acc: 0.9740\n",
      "Epoch 5/100\n",
      "2534/2534 [==============================] - 0s 103us/step - loss: 0.0782 - acc: 0.9759\n",
      "Epoch 6/100\n",
      "2534/2534 [==============================] - 0s 103us/step - loss: 0.0772 - acc: 0.9775\n",
      "Epoch 7/100\n",
      "2534/2534 [==============================] - 0s 107us/step - loss: 0.0743 - acc: 0.9787\n",
      "Epoch 8/100\n",
      "2534/2534 [==============================] - 0s 108us/step - loss: 0.0728 - acc: 0.9779\n",
      "Epoch 9/100\n",
      "2534/2534 [==============================] - 0s 110us/step - loss: 0.0718 - acc: 0.9783\n",
      "Epoch 10/100\n",
      "2534/2534 [==============================] - 0s 106us/step - loss: 0.0715 - acc: 0.9783\n",
      "Epoch 11/100\n",
      "2534/2534 [==============================] - 0s 110us/step - loss: 0.0694 - acc: 0.9767\n",
      "Epoch 12/100\n",
      "2534/2534 [==============================] - 0s 102us/step - loss: 0.0686 - acc: 0.9779\n",
      "Epoch 13/100\n",
      "2534/2534 [==============================] - 0s 126us/step - loss: 0.0677 - acc: 0.9791\n",
      "Epoch 14/100\n",
      "2534/2534 [==============================] - 0s 118us/step - loss: 0.0682 - acc: 0.9791\n",
      "Epoch 15/100\n",
      "2534/2534 [==============================] - 0s 110us/step - loss: 0.0676 - acc: 0.9783\n",
      "Epoch 16/100\n",
      "2534/2534 [==============================] - 0s 122us/step - loss: 0.0666 - acc: 0.9795\n",
      "Epoch 17/100\n",
      "2534/2534 [==============================] - 0s 101us/step - loss: 0.0650 - acc: 0.9779\n",
      "Epoch 18/100\n",
      "2534/2534 [==============================] - 0s 98us/step - loss: 0.0652 - acc: 0.9795\n",
      "Epoch 19/100\n",
      "2534/2534 [==============================] - 0s 111us/step - loss: 0.0635 - acc: 0.9803\n",
      "Epoch 20/100\n",
      "2534/2534 [==============================] - 0s 115us/step - loss: 0.0639 - acc: 0.9795\n",
      "Epoch 21/100\n",
      "2534/2534 [==============================] - 0s 108us/step - loss: 0.0636 - acc: 0.9811\n",
      "Epoch 22/100\n",
      "2534/2534 [==============================] - 0s 97us/step - loss: 0.0621 - acc: 0.9791\n",
      "Epoch 23/100\n",
      "2534/2534 [==============================] - 0s 141us/step - loss: 0.0627 - acc: 0.9803\n",
      "Epoch 24/100\n",
      "2534/2534 [==============================] - 0s 110us/step - loss: 0.0618 - acc: 0.9795\n",
      "Epoch 25/100\n",
      "2534/2534 [==============================] - 0s 112us/step - loss: 0.0614 - acc: 0.9807\n",
      "Epoch 26/100\n",
      "2534/2534 [==============================] - 0s 108us/step - loss: 0.0610 - acc: 0.9803\n",
      "Epoch 27/100\n",
      "2534/2534 [==============================] - 0s 113us/step - loss: 0.0599 - acc: 0.9807\n",
      "Epoch 28/100\n",
      "2534/2534 [==============================] - 0s 115us/step - loss: 0.0599 - acc: 0.9815\n",
      "Epoch 29/100\n",
      "2534/2534 [==============================] - 0s 104us/step - loss: 0.0583 - acc: 0.9822\n",
      "Epoch 30/100\n",
      "2534/2534 [==============================] - 0s 113us/step - loss: 0.0586 - acc: 0.9818\n",
      "Epoch 31/100\n",
      "2534/2534 [==============================] - 0s 110us/step - loss: 0.0569 - acc: 0.9830\n",
      "Epoch 32/100\n",
      "2534/2534 [==============================] - 0s 108us/step - loss: 0.0579 - acc: 0.9834\n",
      "Epoch 33/100\n",
      "2534/2534 [==============================] - 0s 134us/step - loss: 0.0583 - acc: 0.9830\n",
      "Epoch 34/100\n",
      "2534/2534 [==============================] - 0s 145us/step - loss: 0.0559 - acc: 0.9830\n",
      "Epoch 35/100\n",
      "2534/2534 [==============================] - 0s 114us/step - loss: 0.0556 - acc: 0.9826\n",
      "Epoch 36/100\n",
      "2534/2534 [==============================] - 0s 115us/step - loss: 0.0554 - acc: 0.9811\n",
      "Epoch 37/100\n",
      "2534/2534 [==============================] - 0s 135us/step - loss: 0.0564 - acc: 0.9822\n",
      "Epoch 38/100\n",
      "2534/2534 [==============================] - 0s 115us/step - loss: 0.0549 - acc: 0.9850\n",
      "Epoch 39/100\n",
      "2534/2534 [==============================] - 0s 114us/step - loss: 0.0562 - acc: 0.9838\n",
      "Epoch 40/100\n",
      "2534/2534 [==============================] - 0s 138us/step - loss: 0.0543 - acc: 0.9822\n",
      "Epoch 41/100\n",
      "2534/2534 [==============================] - 0s 160us/step - loss: 0.0546 - acc: 0.9826\n",
      "Epoch 42/100\n",
      "2534/2534 [==============================] - 0s 146us/step - loss: 0.0523 - acc: 0.9826\n",
      "Epoch 43/100\n",
      "2534/2534 [==============================] - 0s 132us/step - loss: 0.0531 - acc: 0.9838\n",
      "Epoch 44/100\n",
      "2534/2534 [==============================] - 0s 133us/step - loss: 0.0525 - acc: 0.9838\n",
      "Epoch 45/100\n",
      "2534/2534 [==============================] - 0s 158us/step - loss: 0.0528 - acc: 0.9842\n",
      "Epoch 46/100\n",
      "2534/2534 [==============================] - 0s 133us/step - loss: 0.0528 - acc: 0.9838\n",
      "Epoch 47/100\n",
      "2534/2534 [==============================] - 0s 113us/step - loss: 0.0506 - acc: 0.9830\n",
      "Epoch 48/100\n",
      "2534/2534 [==============================] - 0s 119us/step - loss: 0.0513 - acc: 0.9854\n",
      "Epoch 49/100\n",
      "2534/2534 [==============================] - 0s 139us/step - loss: 0.0496 - acc: 0.9850\n",
      "Epoch 50/100\n",
      "2534/2534 [==============================] - 0s 123us/step - loss: 0.0502 - acc: 0.9826\n",
      "Epoch 51/100\n",
      "2534/2534 [==============================] - 0s 110us/step - loss: 0.0508 - acc: 0.9854\n",
      "Epoch 52/100\n",
      "2534/2534 [==============================] - 0s 115us/step - loss: 0.0494 - acc: 0.9850\n",
      "Epoch 53/100\n",
      "2534/2534 [==============================] - 0s 120us/step - loss: 0.0499 - acc: 0.9838\n",
      "Epoch 54/100\n",
      "2534/2534 [==============================] - 0s 119us/step - loss: 0.0498 - acc: 0.9830\n",
      "Epoch 55/100\n",
      "2534/2534 [==============================] - 0s 114us/step - loss: 0.0503 - acc: 0.9842\n",
      "Epoch 56/100\n",
      "2534/2534 [==============================] - 0s 112us/step - loss: 0.0499 - acc: 0.9834\n",
      "Epoch 57/100\n",
      "2534/2534 [==============================] - 0s 117us/step - loss: 0.0474 - acc: 0.9874\n",
      "Epoch 58/100\n",
      "2534/2534 [==============================] - 0s 116us/step - loss: 0.0478 - acc: 0.9854\n",
      "Epoch 59/100\n",
      "2534/2534 [==============================] - 0s 116us/step - loss: 0.0478 - acc: 0.9846\n",
      "Epoch 60/100\n",
      "2534/2534 [==============================] - 0s 113us/step - loss: 0.0456 - acc: 0.9862\n",
      "Epoch 61/100\n",
      "2534/2534 [==============================] - 0s 118us/step - loss: 0.0484 - acc: 0.9858\n",
      "Epoch 62/100\n",
      "2534/2534 [==============================] - 0s 112us/step - loss: 0.0475 - acc: 0.9842\n",
      "Epoch 63/100\n",
      "2534/2534 [==============================] - 0s 112us/step - loss: 0.0483 - acc: 0.9842\n",
      "Epoch 64/100\n",
      "2534/2534 [==============================] - 0s 137us/step - loss: 0.0470 - acc: 0.9850\n",
      "Epoch 65/100\n",
      "2534/2534 [==============================] - 0s 124us/step - loss: 0.0460 - acc: 0.9854\n",
      "Epoch 66/100\n",
      "2534/2534 [==============================] - 0s 123us/step - loss: 0.0459 - acc: 0.9854\n",
      "Epoch 67/100\n",
      "2534/2534 [==============================] - 0s 136us/step - loss: 0.0467 - acc: 0.9838\n",
      "Epoch 68/100\n",
      "2534/2534 [==============================] - 0s 124us/step - loss: 0.0441 - acc: 0.9870\n",
      "Epoch 69/100\n",
      "2534/2534 [==============================] - 0s 145us/step - loss: 0.0462 - acc: 0.9854\n",
      "Epoch 70/100\n",
      "2534/2534 [==============================] - 0s 143us/step - loss: 0.0450 - acc: 0.9870\n",
      "Epoch 71/100\n",
      "2534/2534 [==============================] - 0s 135us/step - loss: 0.0436 - acc: 0.9854\n",
      "Epoch 72/100\n",
      "2534/2534 [==============================] - 0s 148us/step - loss: 0.0429 - acc: 0.9874\n",
      "Epoch 73/100\n",
      "2534/2534 [==============================] - 0s 152us/step - loss: 0.0458 - acc: 0.9858\n",
      "Epoch 74/100\n",
      "2534/2534 [==============================] - 0s 118us/step - loss: 0.0444 - acc: 0.9870\n",
      "Epoch 75/100\n",
      "2534/2534 [==============================] - 0s 121us/step - loss: 0.0430 - acc: 0.9874\n",
      "Epoch 76/100\n",
      "2534/2534 [==============================] - 0s 119us/step - loss: 0.0438 - acc: 0.9878\n",
      "Epoch 77/100\n",
      "2534/2534 [==============================] - 0s 121us/step - loss: 0.0425 - acc: 0.9874\n",
      "Epoch 78/100\n",
      "2534/2534 [==============================] - 0s 119us/step - loss: 0.0395 - acc: 0.9882\n",
      "Epoch 79/100\n",
      "2534/2534 [==============================] - 0s 121us/step - loss: 0.0414 - acc: 0.9893\n",
      "Epoch 80/100\n",
      "2534/2534 [==============================] - 0s 150us/step - loss: 0.0410 - acc: 0.9874\n",
      "Epoch 81/100\n",
      "2534/2534 [==============================] - 0s 129us/step - loss: 0.0411 - acc: 0.9882\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 0s 136us/step - loss: 0.0405 - acc: 0.9878\n",
      "Epoch 83/100\n",
      "2534/2534 [==============================] - 0s 110us/step - loss: 0.0401 - acc: 0.9882\n",
      "Epoch 84/100\n",
      "2534/2534 [==============================] - 0s 105us/step - loss: 0.0392 - acc: 0.9886\n",
      "Epoch 85/100\n",
      "2534/2534 [==============================] - 0s 109us/step - loss: 0.0371 - acc: 0.9901\n",
      "Epoch 86/100\n",
      "2534/2534 [==============================] - 0s 108us/step - loss: 0.0399 - acc: 0.9878\n",
      "Epoch 87/100\n",
      "2534/2534 [==============================] - 0s 110us/step - loss: 0.0404 - acc: 0.9890\n",
      "Epoch 88/100\n",
      "2534/2534 [==============================] - 0s 105us/step - loss: 0.0377 - acc: 0.9901\n",
      "Epoch 89/100\n",
      "2534/2534 [==============================] - 0s 105us/step - loss: 0.0388 - acc: 0.9874\n",
      "Epoch 90/100\n",
      "2534/2534 [==============================] - 0s 106us/step - loss: 0.0389 - acc: 0.9878\n",
      "Epoch 91/100\n",
      "2534/2534 [==============================] - 0s 108us/step - loss: 0.0362 - acc: 0.9886\n",
      "Epoch 92/100\n",
      "2534/2534 [==============================] - 0s 111us/step - loss: 0.0364 - acc: 0.9890\n",
      "Epoch 93/100\n",
      "2534/2534 [==============================] - 0s 112us/step - loss: 0.0360 - acc: 0.9905\n",
      "Epoch 94/100\n",
      "2534/2534 [==============================] - 0s 110us/step - loss: 0.0357 - acc: 0.9893\n",
      "Epoch 95/100\n",
      "2534/2534 [==============================] - 0s 108us/step - loss: 0.0343 - acc: 0.9901\n",
      "Epoch 96/100\n",
      "2534/2534 [==============================] - 0s 137us/step - loss: 0.0359 - acc: 0.9909\n",
      "Epoch 97/100\n",
      "2534/2534 [==============================] - 0s 116us/step - loss: 0.0355 - acc: 0.9897\n",
      "Epoch 98/100\n",
      "2534/2534 [==============================] - 0s 115us/step - loss: 0.0358 - acc: 0.9893\n",
      "Epoch 99/100\n",
      "2534/2534 [==============================] - 0s 113us/step - loss: 0.0345 - acc: 0.9901\n",
      "Epoch 100/100\n",
      "2534/2534 [==============================] - 0s 112us/step - loss: 0.0358 - acc: 0.9886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6e1cb822e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 11, activation = 'relu',kernel_initializer = 'uniform', input_shape = (20,)))\n",
    "classifier.add(Dense(units = 11, activation = 'relu', kernel_initializer = 'uniform'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer = 'uniform', input_shape = (20,)))\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier.fit(x_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network is now built and trained, the weights are set, now we can evaluate the generalization performance of the neural network on the testing dataset. We then use the threshold of 0.50 to convert output probilities from the sigmoid activation function into binary predictions for gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(x_test)\n",
    "y_pred = (y_pred > 0.5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the neural network using a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[296   5]\n",
      " [ 10 323]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more fashionable plot, recall that 0 is female and 1 is male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f6e1cbcac50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results on the testing dataset are fantastic the confusion matrix shows that the neural net predicts gender on the testing dataset with 98.3% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
